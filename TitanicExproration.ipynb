{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TitanicExproration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "11bskMF4ikltXaCXi9bn-VTa3Rdbp5ffs",
      "authorship_tag": "ABX9TyMWlMZrcwmmEfxOB5tl+oAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaCpotato/kaggle-notebooks/blob/master/TitanicExproration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9fR6osZcJ4-"
      },
      "source": [
        "## Titanic Exproration\n",
        "### link\n",
        "https://www.kaggle.com/c/titanic\n",
        "\n",
        "### reference\n",
        "\n",
        "### Discription\n",
        "This notebook is exploration of titanic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn7TqLs4c1xH",
        "outputId": "b059dbbd-4c33-4c69-adb0-df6e951d2a57"
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADWlAQdUdWH4"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0WgMyh8ddo6"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI83MnTddhCY"
      },
      "source": [
        "!kaggle competitions download -c titanic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDygYMS6eRZC"
      },
      "source": [
        "!unzip /content/train.csv\n",
        "!unzip /content/test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSUniubLfGkm"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns # data visualization\n",
        "\n",
        "###################################use for kaggle notebooks######################################\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "#\n",
        "#import os\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "#\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I40u5YG3s59n"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi9tzyjtfgS_"
      },
      "source": [
        "# Read Train Data\n",
        "train_data = pd.read_csv(\"/content/train.csv\")\n",
        "display(train_data.head())\n",
        "\n",
        "#import regression module \n",
        "from pycaret.regression import * \n",
        "\n",
        "#intialize the setup (in Notebook env)\n",
        "exp_reg = setup(train_data, target = 'Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhGIhSMgOGM"
      },
      "source": [
        "best = compare_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkAfy9ML1GkF"
      },
      "source": [
        "# train multiple lightgbm models with n learning_rate\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "lbl = preprocessing.LabelEncoder()\n",
        "\n",
        "\n",
        "lgbms = create_model('lightgbm', learning_rate = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCS7mZ7i1ccJ"
      },
      "source": [
        "# tune hyperparameters to optimize MAE\n",
        "tuned_lgbms = tune_model(lgbms, optimize = 'MAE') #default is 'R2'\n",
        "# tune hyperparameters with custom_grid\n",
        "params = {\"max_depth\": np.random.randint(1, (len(data.columns)*.85),20),\n",
        "          \"max_features\": np.random.randint(1, len(data.columns),20),\n",
        "          \"min_samples_leaf\": [2,3,4,5,6],\n",
        "          \"criterion\": [\"gini\", \"entropy\"]\n",
        "          }\n",
        "tuned_lgbms_custom = tune_model(lgbms, custom_grid = params)\n",
        "# tune multiple models dynamically\n",
        "top3 = compare_models(n_select = 3)\n",
        "tuned_top3 = [tune_model(i) for i in top3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDlp_yMBgThL"
      },
      "source": [
        "test_data = pd.read_csv(\"/content/test.csv\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8_r20by0vrE"
      },
      "source": [
        "lr_pred_new = predict_model(best, data = test_data)"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}